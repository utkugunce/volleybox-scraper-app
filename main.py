"""
Volleybox Scraper â€” CLI Tool
women.volleybox.net sitesinden veri Ã§ekme aracÄ±.

KullanÄ±m:
    python main.py players --list                       # Oyuncu listesi
    python main.py players --url <url>                  # Tek oyuncu detayÄ±
    python main.py players --list --detail              # Oyuncu listesi + detaylar
    python main.py teams --list                         # TakÄ±m listesi
    python main.py teams --url <url>                    # Tek takÄ±m detayÄ±
    python main.py teams --url <url> --roster           # TakÄ±m kadrosu (flat list)
    python main.py tournaments --list                   # Turnuva listesi
    python main.py tournaments --url <url>              # Turnuva detayÄ±
    python main.py transfers                            # Transferler
    python main.py search <arama terimi>                # Sitede arama

Ortak opsiyonlar:
    --format json|csv|excel                             # Export formatÄ± (default: json)
    --output <dosya>                                    # Ã‡Ä±ktÄ± dosya adÄ±
    --lang tr|en                                        # Site dili (default: tr)
    --pages <n>                                         # Sayfa limiti (default: 5)
    --limit <n>                                         # Detay limiti (default: 10)
"""

import argparse
import sys
from rich.console import Console
from rich.panel import Panel

from scraper.core import VolleyboxScraper
from scraper.players import scrape_player_list, scrape_player_profile, scrape_players_detail
from scraper.teams import scrape_team_list, scrape_team_profile, scrape_teams_detail
from scraper.tournaments import scrape_tournament_list, scrape_tournament_detail
from scraper.transfers import scrape_transfers
from scraper.exporter import export_data, print_summary

console = Console()


def search_site(scraper, query):
    """Search the site using the search bar."""
    console.print(f"[bold cyan]ğŸ” AranÄ±yor: {query}[/bold cyan]")

    url = f"https://women.volleybox.net/{scraper.lang}/search"
    soup = scraper.get_page(url, params={"q": query})

    if not soup:
        return []

    results = []

    # Find all result links
    for link in soup.select("a"):
        href = link.get("href", "")
        text = link.get_text(strip=True)

        if not text or len(text) < 2:
            continue

        import re
        result_type = None
        if re.search(r'-p\d+$', href):
            result_type = "player"
        elif re.search(r'-t\d+$', href):
            result_type = "team"
        elif re.search(r'-c\d+$', href):
            result_type = "tournament"
        else:
            continue

        full_url = href if href.startswith("http") else f"https://women.volleybox.net{href}"
        results.append({
            "name": text,
            "type": result_type,
            "url": full_url,
        })

    # Deduplicate
    seen = set()
    unique = []
    for r in results:
        if r["url"] not in seen:
            seen.add(r["url"])
            unique.append(r)

    console.print(f"[bold green]âœ“ {len(unique)} sonuÃ§ bulundu[/bold green]")
    return unique


def main():
    parser = argparse.ArgumentParser(
        description="ğŸ Volleybox Scraper â€” women.volleybox.net veri Ã§ekme aracÄ±",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )

    subparsers = parser.add_subparsers(dest="command", help="Komut seÃ§in")

    # --- Players ---
    players_parser = subparsers.add_parser("players", help="Oyuncu verileri")
    players_parser.add_argument("--list", action="store_true", help="Oyuncu listesi Ã§ek")
    players_parser.add_argument("--url", type=str, help="Tek oyuncu profili URL")
    players_parser.add_argument("--detail", action="store_true", help="Liste + detay Ã§ek")

    # --- Teams ---
    teams_parser = subparsers.add_parser("teams", help="TakÄ±m verileri")
    teams_parser.add_argument("--list", action="store_true", help="TakÄ±m listesi Ã§ek")
    teams_parser.add_argument("--url", type=str, help="Tek takÄ±m profili URL")
    teams_parser.add_argument("--detail", action="store_true", help="Liste + detay Ã§ek")
    teams_parser.add_argument("--roster", action="store_true", help="Sadece oyuncu kadrosunu Ã§ek (Liste formatÄ±nda)")

    # --- Tournaments ---
    tourn_parser = subparsers.add_parser("tournaments", help="Turnuva verileri")
    tourn_parser.add_argument("--list", action="store_true", help="Turnuva listesi Ã§ek")
    tourn_parser.add_argument("--url", type=str, help="Tek turnuva URL")
    tourn_parser.add_argument("--matches", action="store_true", help="Turnuva maÃ§larÄ±nÄ± Ã§ek")

    # --- Transfers ---
    transfer_parser = subparsers.add_parser("transfers", help="Transfer verileri")

    # --- Search ---
    search_parser = subparsers.add_parser("search", help="Sitede arama")
    search_parser.add_argument("query", type=str, help="Arama terimi")

    # --- Common options ---
    for p in [players_parser, teams_parser, tourn_parser, transfer_parser, search_parser]:
        p.add_argument("--format", choices=["json", "csv", "excel"], default="json", help="Export formatÄ±")
        p.add_argument("--output", "-o", type=str, help="Ã‡Ä±ktÄ± dosya adÄ±")
        p.add_argument("--lang", choices=["tr", "en"], default="tr", help="Site dili")
        p.add_argument("--pages", type=int, default=5, help="Sayfa limiti")
        p.add_argument("--limit", type=int, default=10, help="Detay Ã§ekilecek kayÄ±t limiti")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(0)

    # Banner
    console.print(Panel.fit(
        "[bold white]ğŸ Volleybox Scraper[/bold white]\n"
        "[dim]women.volleybox.net veri Ã§ekme aracÄ±[/dim]",
        border_style="bright_magenta",
    ))

    # Create scraper with context manager for proper cleanup
    with VolleyboxScraper(lang=args.lang) as scraper:
        data = []

        # --- Execute command ---
        if args.command == "players":
            if args.url:
                result = scrape_player_profile(scraper, args.url)
                data = [result] if result else []
            elif args.list:
                data = scrape_player_list(scraper, page_limit=args.pages)
                if args.detail and data:
                    data = scrape_players_detail(scraper, data, limit=args.limit)
            else:
                console.print("[yellow]--list veya --url belirtin.[/yellow]")
                sys.exit(1)

        elif args.command == "teams":
            if args.url:
                result = scrape_team_profile(scraper, args.url)
                if result:
                    if args.roster:
                        # Extract roster as main data list
                        roster = result.get("roster", [])
                        team_name = result.get("name", "Unknown Team")
                        # Add team context
                        for p in roster:
                            p["team"] = team_name
                            p["team_url"] = args.url
                        data = roster
                    else:
                        data = [result]
                else:
                    data = []
            elif args.list:
                data = scrape_team_list(scraper, page_limit=args.pages)
                if args.detail and data:
                    data = scrape_teams_detail(scraper, data, limit=args.limit)
            else:
                console.print("[yellow]--list veya --url belirtin.[/yellow]")
                sys.exit(1)

        elif args.command == "tournaments":
            if args.url:
                if args.matches:
                    from scraper.tournaments import scrape_tournament_matches
                    data = scrape_tournament_matches(scraper, args.url)
                else:
                    result = scrape_tournament_detail(scraper, args.url)
                    data = [result] if result else []
            elif args.list:
                data = scrape_tournament_list(scraper, page_limit=args.pages)
            else:
                console.print("[yellow]--list veya --url belirtin.[/yellow]")
                sys.exit(1)

        elif args.command == "transfers":
            data = scrape_transfers(scraper, page_limit=args.pages)

        elif args.command == "search":
            data = search_site(scraper, args.query)

    # --- Output ---
    if data:
        print_summary(data, title=f"{args.command.upper()} SonuÃ§larÄ±")

        if args.output:
            export_data(data, args.output, format=args.format)
        else:
            default_name = f"volleybox_{args.command}"
            export_data(data, default_name, format=args.format)
    else:
        console.print("[yellow]âš  Veri bulunamadÄ±.[/yellow]")


if __name__ == "__main__":
    main()
